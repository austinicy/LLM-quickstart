{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90c6730f-5d76-450b-9788-ec883d024f57",
   "metadata": {},
   "source": [
    "# Hugging Face Transformers 微调训练入门\n",
    "\n",
    "本示例将介绍基于 Transformers 实现模型微调训练的主要流程，包括：\n",
    "- 数据集下载\n",
    "- 数据预处理\n",
    "- 训练超参数配置\n",
    "- 训练评估指标设置\n",
    "- 训练器基本介绍\n",
    "- 实战训练\n",
    "- 模型保存"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0b1e12-1921-4438-8d5d-9760a629dcfe",
   "metadata": {},
   "source": [
    "## YelpReviewFull 数据集\n",
    "\n",
    "**Hugging Face 数据集：[ YelpReviewFull ](https://huggingface.co/datasets/yelp_review_full)**\n",
    "\n",
    "### 数据集摘要\n",
    "\n",
    "Yelp评论数据集包括来自Yelp的评论。它是从Yelp Dataset Challenge 2015数据中提取的。\n",
    "\n",
    "### 支持的任务和排行榜\n",
    "文本分类、情感分类：该数据集主要用于文本分类：给定文本，预测情感。\n",
    "\n",
    "### 语言\n",
    "这些评论主要以英语编写。\n",
    "\n",
    "### 数据集结构\n",
    "\n",
    "#### 数据实例\n",
    "一个典型的数据点包括文本和相应的标签。\n",
    "\n",
    "来自YelpReviewFull测试集的示例如下：\n",
    "\n",
    "```json\n",
    "{\n",
    "    'label': 0,\n",
    "    'text': 'I got \\'new\\' tires from them and within two weeks got a flat. I took my car to a local mechanic to see if i could get the hole patched, but they said the reason I had a flat was because the previous patch had blown - WAIT, WHAT? I just got the tire and never needed to have it patched? This was supposed to be a new tire. \\\\nI took the tire over to Flynn\\'s and they told me that someone punctured my tire, then tried to patch it. So there are resentful tire slashers? I find that very unlikely. After arguing with the guy and telling him that his logic was far fetched he said he\\'d give me a new tire \\\\\"this time\\\\\". \\\\nI will never go back to Flynn\\'s b/c of the way this guy treated me and the simple fact that they gave me a used tire!'\n",
    "}\n",
    "```\n",
    "\n",
    "#### 数据字段\n",
    "\n",
    "- 'text': 评论文本使用双引号（\"）转义，任何内部双引号都通过2个双引号（\"\"）转义。换行符使用反斜杠后跟一个 \"n\" 字符转义，即 \"\\n\"。\n",
    "- 'label': 对应于评论的分数（介于1和5之间）。\n",
    "\n",
    "#### 数据拆分\n",
    "\n",
    "Yelp评论完整星级数据集是通过随机选取每个1到5星评论的130,000个训练样本和10,000个测试样本构建的。总共有650,000个训练样本和50,000个测试样本。\n",
    "\n",
    "## 下载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbf72d6c-7ea5-4ee1-969a-c5060b9cb2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"yelp_review_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec6fc806-1395-42dd-8121-a6e98a95cf01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 650000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c94ad529-1604-48bd-8c8d-aa2f3bca6200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 0,\n",
       " 'text': \"Owning a driving range inside the city limits is like a license to print money.  I don't think I ask much out of a driving range.  Decent mats, clean balls and accessible hours.  Hell you need even less people now with the advent of the machine that doles out the balls.  This place has none of them.  It is april and there are no grass tees yet.  BTW they opened for the season this week although it has been golfing weather for a month.  The mats look like the carpet at my 107 year old aunt Irene's house.  Worn and thread bare.  Let's talk about the hours.  This place is equipped with lights yet they only sell buckets of balls until 730.  It is still light out.  Finally lets you have the pit to hit into.  When I arrived I wasn't sure if this was a driving range or an excavation site for a mastodon or a strip mining operation.  There is no grass on the range. Just mud.  Makes it a good tool to figure out how far you actually are hitting the ball.  Oh, they are cash only also.\\\\n\\\\nBottom line, this place sucks.  The best hope is that the owner sells it to someone that actually wants to make money and service golfers in Pittsburgh.\"}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dc45997-e391-456f-b0b9-d3193b0f6a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import datasets\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e2ecebb-d5d1-456d-967c-842a79fdd622",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1af560b6-7d21-499e-9b82-114be371a98a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1 star</td>\n",
       "      <td>Ugh!!! The worst Outback I've ever been to.\\nYa know, I go to outback because it's consistent.....Until last Friday!! I can overlook poor service when it's busy...WHEN IT'S BUSY!!! There was like 12 people in this joint including my party of 5. It's close to ASU, so no surprise when you get pretentious, arrogant, \\\"too cool for this crappy job\\\" college kids semi-serving you. My one huge compaint was the food. The ribs were so awful I boxed them up immediately and brought them home to see if my golden retriever could stomach them. He did. But, umm, he's a dog. He licks stuff that I don't. Long and short: don't go to this particular outback unless you want to pay for rude servers and terrible food. And no, I couldn't bring myself to complain to management or server. I'm passive aggressive, so I tipped her customary %20 (like $15) and left, only to get out the ol' yelp machine and start this keyboard-kommando rant about my experience.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2 star</td>\n",
       "      <td>Parked in their parking garage, didn't think our SUV was going to fit, but magically it did.\\n\\nWent up to the top, come to find out that the elevator doesn't go to the top.  Parked the vehicle, go down to the street level, and find the validation machine so we can walk around the \\\"Freemont Experience\\\" for a couple of hours.\\nCasino is old, stale, and just...weird.\\n\\nI appreciate the 3 free hours, but sheesh, the casino needs a makeover ASAP.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5 stars</td>\n",
       "      <td>Wow!  Great food, great service.  Really enjoyed every bite!  Very sophisticated.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3 stars</td>\n",
       "      <td>Yeah!  Tasty Mexican!  Modern decor and an old b/w movie being projected on the wall while we were there. Neat.  The menu is small, but you should be able to find something you like.    There were some fruity looking drinks being served in Mason Jars as well.  I was too chicken to ask what they were!  We were happy to be able to get out Mexican fix even in Montreal.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1 star</td>\n",
       "      <td>Well at first it looked good then it went to hell,  first day decided to eat dinner at the cafe food was good until a bug crawled out of my broccoli  I was eating, I call for the Manger to show him his comment was well sometimes that happens with vegetables but let me comp your dinner for tonight like was going to pay for that tells me that they don't wash and clean the vegetables. \\n\\nLater that night security comes up to my dad and start giving him a hard time telling him he can't  be here and asked him for his Id  when I tell them he is with me and we are staying at the hotel  they tell me stay out of it.\\n\\nNext day we are playing  all day decided to take a break on a sofa upstairs security  came up to him when his legs were crossed  and they smacked  his leg hard enough to knock it to the floor and they told him he had to leave and walked him down stairs , my friends got me to tell me what's going I went to security and asked for the hotel manager was told he was not in and I can talk to a pit boss and head of security,   explained to them wants was going on and why are they picking on a 70 plus Oldman,   was told to write up complaint and they would look into .  I told them that was not good enough they asked what I wanted from them I told them action I was the officers that put their hands on my dad and hit my dad fired,  that told me it is he said she said and the officer just got off shift and is gone funny left pretty Damm quick,  I told them I have 3 witnesses that saw what they did I was told to write it up and they would look into it.  I asked them to look at the camera video  they told me they don't have cameras  in that area I called b.s.  I did what the asked and filled out the paperwork and left it on the table in my room attention management.   Let's see if I get a response,  maybe I should contact  a attorney as that was harassment  and asualt on a elder.\\n\\nNo stars here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1 star</td>\n",
       "      <td>OVERRATED.\\n\\nI know it was reviewed on Food network channel as top 10 something or another in the nation.  Did their quality go down since they know that people will seek it out irregardless of the review?\\n\\nLocation- in down town Phoenix- but in a crappy part that doesn't look safe at night.  Dirt lot in back is not an option to park because of city ordinances or safety issues.\\n\\nWait time- for those reviewers who waited over 1hour in the hot desert sun- i don't know what to say. There is little to no shade, it's hot, you lean on wooden fences... it's a strange system where you sign your name to be called. However, if you sign in, and say you want to go do something else instead of wait an hour in the hot sun, they will cross you off the list if you aren't there for roll call. Some people behind us, just waited a bit then walked in and sat at the bar (where they were seating other people).  And they say they don't play favorites?  Please....  If you can't stand to wait, you can  order take out and get your food before all the other people waiting.  Do they let anyone know? NO. Do they care that people waiting over 1 hour have to wait another hour for food so To-Go people can get their food? NO...\\n\\nFood- maybe the food tastes better because your out in the hot sun,dehydrating, standing, waiting... because the food didn't look like anything special and didn't taste special either.  Just run of the mill breakfast food.  Does it take 45min to make waffles and toast?  We told them we had a flight to catch, and our food took a crazy amount of time to make. No wonder the wait is so long outside! 2oz of OJ was $2.50\\n\\nAmbiance- it's a 3 room (1 kitchen, one bar set up, 1 room with 4 tables and seating on bar-like thing looking outside. Think they can seat ~20 people inside?  Poor decor, crappy furniture.... they must make a ton of money because none of it is going into the location/upkeep/decor.\\n\\nDon't even think about coming here with more than 6 people.... Where will you put them? Wait til the restaurant clears so you have a room to yourself?  ha...\\n\\nOnce again, OVER-RATED.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3 stars</td>\n",
       "      <td>Probably 80 percent of the time my order is correct and timely, I've rarely had a problem with this place and when I have, it doesn't bother me because it is McDonald's.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5 stars</td>\n",
       "      <td>The only place i have to stop by when i visit Phoenix! Food is unbelievable and the desert is so creative! Love it!!!! I wish i was local so i can eat there anytime i want! Seriously just look at the ratings, all of us can't be wrong!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1 star</td>\n",
       "      <td>ooof - I went in on a Saturday, about 2 hours after they'd opened, and there was almost nothing in their cases.  It didn't really look like they'd sold out of stuff - they hade one case with a handful of unappetizing flavors of macaroons (earl grey macaroons??) and another case with five flavors of cupcakes.  I wanted to try the varieties so I got one of each - brought them home and discovered that the cupcake itself was dry and the buttercream was too buttery (I thought that was impossible, but even my husband disliked the frosting - like eating a stick of butter).  The woman who helped me was a little rough around the edges - not exactly rude, but less than professional.  I don't care that I live half a block away - you couldn't pay me to go back.  I'll stick with Barb's, thank you very much!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2 star</td>\n",
       "      <td>Wanting to be fair, I've gone back to Hillside Spot (Is that name godawful and unimaginative or what?) several times, hitting breakfast, lunch and dinner..I guess I desperately want to have something more than chains and fast food near me in Ahwatukee.  I just left there after having dinner again.  They still have lots of work to do.. and need to get some of those  fools out of their kitchen before it's too late....Let's start with breakfast two mornings ago.  Pancakes, two eggs over medium, and bacon( gots to eat the sacred pig you know) .  I do like their pancakes...even if they aren't whole grain.. The over medium eggs were fried on the bottom, fried brown.   The bacon was ice cold. Not warm, not room temperature, cold.  I called someone over and told her my over medium eggs were actually fried on the bottom and she had the nerve to tell me \\\"well you have to fry an egg to cook it.\\\"   I mean do I look like I just fell of the turnip truck? I told her I know what over medium is and these ain't.  Then told her the bacon was cold.  Cold.  She took my plate and came back with another one in 3-4 minutes announcing she had taken someone else's order.  At least the eggs were done right and the bacon was hot.  Ok.. dinner last week was a chicken and house made pasta dish that I thought was rather good.  And worked well as leftovers too.  I like a good pasta..  Dinner tonight...their roasted half chicken is always good, tasty, but the fries that came with it had so much salt on them I thought I was Lot's wife!  I mean bitter salty..How does that happen in a good kitchen? The apple pie was a mixture of warm and cool and mostly big chunks of crust....ug....... I'm batting about .150 here with this joint....I suppose I'll keep trying and see if they ever get it right....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9df7cd0-23cd-458f-b2b5-f025c3b9fe62",
   "metadata": {},
   "source": [
    "## 预处理数据\n",
    "\n",
    "下载数据集到本地后，使用 Tokenizer 来处理文本，对于长度不等的输入数据，可以使用填充（padding）和截断（truncation）策略来处理。\n",
    "\n",
    "Datasets 的 `map` 方法，支持一次性在整个数据集上应用预处理函数。\n",
    "\n",
    "下面使用填充到最大长度的策略，处理整个数据集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bf2b342-e1dd-4ab6-ad57-28eb2513ae38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fa34a77995a4cc2bbb26bafe0337578",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47a415a8-cd15-4a8c-851b-9b4740ef8271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>token_type_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2 star</td>\n",
       "      <td>I tried all the dishes yelp said to order and couldn't finish anything. Everything was too salty to consume.  \\nI gave them two stars because the dessert on the other hand was great. I make back once just to have dessert but probably would never come back again.</td>\n",
       "      <td>[101, 146, 1793, 1155, 1103, 10514, 6798, 1233, 1643, 1163, 1106, 1546, 1105, 1577, 112, 189, 3146, 1625, 119, 5268, 1108, 1315, 6870, 1183, 1106, 17914, 119, 165, 183, 2240, 1522, 1172, 1160, 2940, 1272, 1103, 20392, 1113, 1103, 1168, 1289, 1108, 1632, 119, 146, 1294, 1171, 1517, 1198, 1106, 1138, 20392, 1133, 1930, 1156, 1309, 1435, 1171, 1254, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(tokenized_datasets[\"train\"], num_examples=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c33d153-f729-4f04-972c-a764c1cbbb8b",
   "metadata": {},
   "source": [
    "### 数据抽样\n",
    "\n",
    "使用 1000 个数据样本，在 BERT 上演示小规模训练（基于 Pytorch Trainer）\n",
    "\n",
    "`shuffle()`函数会随机重新排列列的值。如果您希望对用于洗牌数据集的算法有更多控制，可以在此函数中指定generator参数来使用不同的numpy.random.Generator。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a17317d8-3c6a-467f-843d-87491f600db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b65d63-2d3a-4a56-bc31-6e88a29e9dec",
   "metadata": {},
   "source": [
    "## 微调训练配置\n",
    "\n",
    "### 加载 BERT 模型\n",
    "\n",
    "警告通知我们正在丢弃一些权重（`vocab_transform` 和 `vocab_layer_norm` 层），并随机初始化其他一些权重（`pre_classifier` 和 `classifier` 层）。在微调模型情况下是绝对正常的，因为我们正在删除用于预训练模型的掩码语言建模任务的头部，并用一个新的头部替换它，对于这个新头部，我们没有预训练的权重，所以库会警告我们在用它进行推理之前应该对这个模型进行微调，而这正是我们要做的事情。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d2af4df-abd4-4a4b-94b6-b0e7375304ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44014df-b52c-4c72-9e9f-54424725a473",
   "metadata": {},
   "source": [
    "### 训练超参数（TrainingArguments）\n",
    "\n",
    "完整配置参数与默认值：https://huggingface.co/docs/transformers/v4.36.1/en/main_classes/trainer#transformers.TrainingArguments\n",
    "\n",
    "源代码定义：https://github.com/huggingface/transformers/blob/v4.36.1/src/transformers/training_args.py#L161\n",
    "\n",
    "**最重要配置：模型权重保存路径(output_dir)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98c01d5c-de72-4ff0-b11d-e07ac5346888",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "model_dir = \"models/bert-base-cased\"\n",
    "\n",
    "# logging_steps 默认值为500，根据我们的训练数据和步长，将其设置为100\n",
    "training_args = TrainingArguments(output_dir=f\"{model_dir}/test_trainer\",\n",
    "                                  logging_dir=f\"{model_dir}/test_trainer/runs\",\n",
    "                                  logging_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ce03480-3aaa-48ea-a0c6-a177b8d8e34f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=models/bert-base-cased/test_trainer/runs,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=100,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=OptimizerNames.ADAMW_HF,\n",
      "optim_args=None,\n",
      "output_dir=models/bert-base-cased/test_trainer,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=8,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=models/bert-base-cased/test_trainer,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 完整的超参数配置\n",
    "print(training_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebd3365-d359-4ab4-a300-4717590cc240",
   "metadata": {},
   "source": [
    "### 训练过程中的指标评估（Evaluate)\n",
    "\n",
    "**[Hugging Face Evaluate 库](https://huggingface.co/docs/evaluate/index)** 支持使用一行代码，获得数十种不同领域（自然语言处理、计算机视觉、强化学习等）的评估方法。 当前支持 **完整评估指标：https://huggingface.co/evaluate-metric**\n",
    "\n",
    "训练器（Trainer）在训练过程中不会自动评估模型性能。因此，我们需要向训练器传递一个函数来计算和报告指标。 \n",
    "\n",
    "Evaluate库提供了一个简单的准确率函数，您可以使用`evaluate.load`函数加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a8ef138-5bf2-41e5-8c68-df8e11f4e98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d406c0-56d0-4a54-9c6c-e126ab7f5254",
   "metadata": {},
   "source": [
    "\n",
    "接着，调用 `compute` 函数来计算预测的准确率。\n",
    "\n",
    "在将预测传递给 compute 函数之前，我们需要将 logits 转换为预测值（**所有Transformers 模型都返回 logits**）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f46d2e59-1ebf-43d2-bc86-6b57a4d24d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2feba67-9ca9-4793-9a15-3eaa426df2a1",
   "metadata": {},
   "source": [
    "#### 训练过程指标监控\n",
    "\n",
    "通常，为了监控训练过程中的评估指标变化，我们可以在`TrainingArguments`指定`evaluation_strategy`参数，以便在 epoch 结束时报告评估指标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afaaee18-4986-4e39-8ad9-b8d413ab4cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(output_dir=f\"{model_dir}/test_trainer\",\n",
    "                                  evaluation_strategy=\"epoch\", \n",
    "                                  logging_dir=f\"{model_dir}/test_trainer/runs\",\n",
    "                                  logging_steps=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47d6981-e444-4c0f-a7cb-dd7f2ba8df12",
   "metadata": {},
   "source": [
    "## 开始训练\n",
    "\n",
    "### 实例化训练器（Trainer）\n",
    "\n",
    "`kernel version` 版本问题：暂不影响本示例代码运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca1d12ac-89dc-4c30-8282-f859724c0062",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a833e0db-1168-4a3c-8b75-bfdcef8c5157",
   "metadata": {},
   "source": [
    "## 使用 nvidia-smi 查看 GPU 使用\n",
    "\n",
    "为了实时查看GPU使用情况，可以使用 `watch` 指令实现轮询：`watch -n 1 nvidia-smi`:\n",
    "\n",
    "```shell\n",
    "Every 1.0s: nvidia-smi                                                   Wed Dec 20 14:37:41 2023\n",
    "\n",
    "Wed Dec 20 14:37:41 2023\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  Tesla T4                       Off | 00000000:00:0D.0 Off |                    0 |\n",
    "| N/A   64C    P0              69W /  70W |   6665MiB / 15360MiB |     98%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "|    0   N/A  N/A     18395      C   /root/miniconda3/bin/python                6660MiB |\n",
    "+---------------------------------------------------------------------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "accfe921-471d-481a-96da-c491cdebad0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zheng\\anaconda3\\envs\\llmquick\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 02:02, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.568800</td>\n",
       "      <td>1.222721</td>\n",
       "      <td>0.490000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.106800</td>\n",
       "      <td>0.982350</td>\n",
       "      <td>0.586000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.893500</td>\n",
       "      <td>1.060765</td>\n",
       "      <td>0.578000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=375, training_loss=1.0843751220703124, metrics={'train_runtime': 123.6096, 'train_samples_per_second': 24.27, 'train_steps_per_second': 3.034, 'total_flos': 789354427392000.0, 'train_loss': 1.0843751220703124, 'epoch': 3.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d581099-37a4-4470-b051-1ada38554089",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_test_dataset = tokenized_datasets[\"test\"].shuffle(seed=64).select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ffb47eab-1370-491e-8a84-6d5347a350b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='398' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 16:50:25]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.038762092590332,\n",
       " 'eval_accuracy': 0.56,\n",
       " 'eval_runtime': 1.3026,\n",
       " 'eval_samples_per_second': 76.768,\n",
       " 'eval_steps_per_second': 9.98,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(small_test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a55686-7c43-4ab8-a5cd-0e77f14c7c52",
   "metadata": {},
   "source": [
    "### 保存模型和训练状态\n",
    "\n",
    "- 使用 `trainer.save_model` 方法保存模型，后续可以通过 from_pretrained() 方法重新加载\n",
    "- 使用 `trainer.save_state` 方法保存训练状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad0cbc14-9ef7-450f-a1a3-4f92b6486f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(f\"{model_dir}/finetuned-trainer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "badf5868-2847-439d-a73e-42d1cca67b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61828934-01da-4fc3-9e75-8d754c25dfbc",
   "metadata": {},
   "source": [
    "## Homework: 使用完整的 YelpReviewFull 数据集训练，对比看 Acc 最高能到多少"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32318ac-fc7f-441f-8b1a-dbe9fe8204ac",
   "metadata": {},
   "source": [
    "### 准备数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "60e8724a-bacc-4cb3-af8f-e9926eca8b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_dataset = tokenized_datasets[\"train\"]\n",
    "full_test_dataset_unprocess = tokenized_datasets[\"test\"].train_test_split(test_size=0.4)\n",
    "full_eval_dataset = full_test_dataset_unprocess[\"train\"]\n",
    "full_test_dataset = full_test_dataset_unprocess[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "60ba1528-a1ba-4b49-8dc8-dbddb654a7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(output_dir=f\"{model_dir}/test_trainer\",\n",
    "                                  logging_dir=f\"{model_dir}/test_trainer/runs\",\n",
    "                                  logging_steps=1000,\n",
    "                                  save_strategy=\"no\",\n",
    "                                  save_steps=40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3b733993-1924-4f3d-94d9-162f4e094e13",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=models/bert-base-cased/test_trainer/runs,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=1000,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=OptimizerNames.ADAMW_HF,\n",
      "optim_args=None,\n",
      "output_dir=models/bert-base-cased/test_trainer,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=8,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=models/bert-base-cased/test_trainer,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=40000,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 完整的超参数配置\n",
    "print(training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "485b8d9e-8d90-4a96-ad37-90d4976d4607",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_full = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=full_train_dataset,\n",
    "    eval_dataset=full_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "78dc7db4-5596-4c0d-b936-f3e8de665400",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zheng\\anaconda3\\envs\\llmquick\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='243750' max='243750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [243750/243750 16:43:21, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.108400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.033700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.983900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.964800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.956800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.940700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.933000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.931100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.909800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.940100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.916300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.919400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.915100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.911500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.914800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.930200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.900200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.912700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.917400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.898300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.892600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.897600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.916400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.895100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.903700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.907800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.914600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.926600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.893600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.890900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.900900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.894200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.907600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.890800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.870500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.914900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>0.880500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.893900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>0.889700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.914900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>0.908700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.881500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>0.892500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.875500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>0.879800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.894100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47000</td>\n",
       "      <td>0.887200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.866000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49000</td>\n",
       "      <td>0.884200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.886500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51000</td>\n",
       "      <td>0.892900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>0.887400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53000</td>\n",
       "      <td>0.872600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>0.873900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>0.899100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>0.892500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57000</td>\n",
       "      <td>0.873100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58000</td>\n",
       "      <td>0.876300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59000</td>\n",
       "      <td>0.872800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.884300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61000</td>\n",
       "      <td>0.872700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62000</td>\n",
       "      <td>0.864200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63000</td>\n",
       "      <td>0.877100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64000</td>\n",
       "      <td>0.865500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65000</td>\n",
       "      <td>0.877400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66000</td>\n",
       "      <td>0.870500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67000</td>\n",
       "      <td>0.845700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68000</td>\n",
       "      <td>0.874600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69000</td>\n",
       "      <td>0.869800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>0.873300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71000</td>\n",
       "      <td>0.868700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72000</td>\n",
       "      <td>0.866600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73000</td>\n",
       "      <td>0.868800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74000</td>\n",
       "      <td>0.861900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75000</td>\n",
       "      <td>0.873700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76000</td>\n",
       "      <td>0.868800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77000</td>\n",
       "      <td>0.870300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78000</td>\n",
       "      <td>0.856100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79000</td>\n",
       "      <td>0.878300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80000</td>\n",
       "      <td>0.881200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81000</td>\n",
       "      <td>0.863700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82000</td>\n",
       "      <td>0.849700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83000</td>\n",
       "      <td>0.827400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84000</td>\n",
       "      <td>0.820200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85000</td>\n",
       "      <td>0.808900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86000</td>\n",
       "      <td>0.850800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87000</td>\n",
       "      <td>0.837700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88000</td>\n",
       "      <td>0.811500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89000</td>\n",
       "      <td>0.821200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90000</td>\n",
       "      <td>0.819500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91000</td>\n",
       "      <td>0.831300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92000</td>\n",
       "      <td>0.816700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93000</td>\n",
       "      <td>0.823600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94000</td>\n",
       "      <td>0.806500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95000</td>\n",
       "      <td>0.831100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96000</td>\n",
       "      <td>0.826200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97000</td>\n",
       "      <td>0.822100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98000</td>\n",
       "      <td>0.815100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99000</td>\n",
       "      <td>0.844000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100000</td>\n",
       "      <td>0.810300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101000</td>\n",
       "      <td>0.828300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102000</td>\n",
       "      <td>0.819900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103000</td>\n",
       "      <td>0.809500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104000</td>\n",
       "      <td>0.799700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105000</td>\n",
       "      <td>0.816700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106000</td>\n",
       "      <td>0.798200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107000</td>\n",
       "      <td>0.808500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108000</td>\n",
       "      <td>0.821100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109000</td>\n",
       "      <td>0.835100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110000</td>\n",
       "      <td>0.803100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111000</td>\n",
       "      <td>0.835300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112000</td>\n",
       "      <td>0.823800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113000</td>\n",
       "      <td>0.841100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114000</td>\n",
       "      <td>0.814200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115000</td>\n",
       "      <td>0.807500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116000</td>\n",
       "      <td>0.818200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117000</td>\n",
       "      <td>0.822100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118000</td>\n",
       "      <td>0.817100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119000</td>\n",
       "      <td>0.836000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120000</td>\n",
       "      <td>0.833100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121000</td>\n",
       "      <td>0.815000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122000</td>\n",
       "      <td>0.816300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123000</td>\n",
       "      <td>0.819300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124000</td>\n",
       "      <td>0.815200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125000</td>\n",
       "      <td>0.821500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126000</td>\n",
       "      <td>0.817200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127000</td>\n",
       "      <td>0.810000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128000</td>\n",
       "      <td>0.819800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129000</td>\n",
       "      <td>0.823600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130000</td>\n",
       "      <td>0.809100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131000</td>\n",
       "      <td>0.817000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132000</td>\n",
       "      <td>0.813600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133000</td>\n",
       "      <td>0.816500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134000</td>\n",
       "      <td>0.816600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135000</td>\n",
       "      <td>0.812000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136000</td>\n",
       "      <td>0.823600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137000</td>\n",
       "      <td>0.813500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138000</td>\n",
       "      <td>0.874100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139000</td>\n",
       "      <td>0.848600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140000</td>\n",
       "      <td>0.828700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141000</td>\n",
       "      <td>0.839900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142000</td>\n",
       "      <td>0.858200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143000</td>\n",
       "      <td>0.864600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144000</td>\n",
       "      <td>0.937200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145000</td>\n",
       "      <td>0.849300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146000</td>\n",
       "      <td>0.824100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147000</td>\n",
       "      <td>0.839200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148000</td>\n",
       "      <td>0.839100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149000</td>\n",
       "      <td>0.852700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150000</td>\n",
       "      <td>0.846200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151000</td>\n",
       "      <td>0.878900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152000</td>\n",
       "      <td>0.852800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153000</td>\n",
       "      <td>0.884800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154000</td>\n",
       "      <td>0.878200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155000</td>\n",
       "      <td>0.812500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156000</td>\n",
       "      <td>0.807200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157000</td>\n",
       "      <td>0.786400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158000</td>\n",
       "      <td>0.779400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159000</td>\n",
       "      <td>0.777600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160000</td>\n",
       "      <td>0.799500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161000</td>\n",
       "      <td>0.791000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162000</td>\n",
       "      <td>0.780200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163000</td>\n",
       "      <td>0.770700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164000</td>\n",
       "      <td>0.751600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165000</td>\n",
       "      <td>0.747400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166000</td>\n",
       "      <td>0.756100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167000</td>\n",
       "      <td>0.742000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168000</td>\n",
       "      <td>0.767200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169000</td>\n",
       "      <td>0.768700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170000</td>\n",
       "      <td>0.766700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171000</td>\n",
       "      <td>0.751300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172000</td>\n",
       "      <td>0.756900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173000</td>\n",
       "      <td>0.753400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174000</td>\n",
       "      <td>0.823200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175000</td>\n",
       "      <td>0.773900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176000</td>\n",
       "      <td>0.750900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177000</td>\n",
       "      <td>0.752400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178000</td>\n",
       "      <td>0.754700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179000</td>\n",
       "      <td>0.748700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180000</td>\n",
       "      <td>0.742500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181000</td>\n",
       "      <td>0.748700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182000</td>\n",
       "      <td>0.747300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183000</td>\n",
       "      <td>0.740500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184000</td>\n",
       "      <td>0.745300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185000</td>\n",
       "      <td>0.748700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186000</td>\n",
       "      <td>0.747400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187000</td>\n",
       "      <td>0.735300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188000</td>\n",
       "      <td>0.746200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189000</td>\n",
       "      <td>0.738600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190000</td>\n",
       "      <td>0.726700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191000</td>\n",
       "      <td>0.736100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192000</td>\n",
       "      <td>0.718700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193000</td>\n",
       "      <td>0.737400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194000</td>\n",
       "      <td>0.738700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195000</td>\n",
       "      <td>0.724300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196000</td>\n",
       "      <td>0.741400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197000</td>\n",
       "      <td>0.717000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198000</td>\n",
       "      <td>0.739300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199000</td>\n",
       "      <td>0.731500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200000</td>\n",
       "      <td>0.728600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>201000</td>\n",
       "      <td>0.745800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>202000</td>\n",
       "      <td>0.729800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>203000</td>\n",
       "      <td>0.722900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>204000</td>\n",
       "      <td>0.730000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205000</td>\n",
       "      <td>0.740400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>206000</td>\n",
       "      <td>0.735100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>207000</td>\n",
       "      <td>0.724500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208000</td>\n",
       "      <td>0.728000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>209000</td>\n",
       "      <td>0.736100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210000</td>\n",
       "      <td>0.729600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>211000</td>\n",
       "      <td>0.731300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>212000</td>\n",
       "      <td>0.706400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>213000</td>\n",
       "      <td>0.721900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>214000</td>\n",
       "      <td>0.735600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215000</td>\n",
       "      <td>0.743600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>216000</td>\n",
       "      <td>0.719900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>217000</td>\n",
       "      <td>0.703100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>218000</td>\n",
       "      <td>0.720500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>219000</td>\n",
       "      <td>0.708000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220000</td>\n",
       "      <td>0.721200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>221000</td>\n",
       "      <td>0.730100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>222000</td>\n",
       "      <td>0.714900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>223000</td>\n",
       "      <td>0.724800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224000</td>\n",
       "      <td>0.709200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225000</td>\n",
       "      <td>0.714800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>226000</td>\n",
       "      <td>0.704500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>227000</td>\n",
       "      <td>0.711400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>228000</td>\n",
       "      <td>0.736100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>229000</td>\n",
       "      <td>0.707100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230000</td>\n",
       "      <td>0.703100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>231000</td>\n",
       "      <td>0.711600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>232000</td>\n",
       "      <td>0.709900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>233000</td>\n",
       "      <td>0.720600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>234000</td>\n",
       "      <td>0.716200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235000</td>\n",
       "      <td>0.711900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>236000</td>\n",
       "      <td>0.712900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>237000</td>\n",
       "      <td>0.716600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>238000</td>\n",
       "      <td>0.703400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>239000</td>\n",
       "      <td>0.709100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240000</td>\n",
       "      <td>0.709800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>241000</td>\n",
       "      <td>0.693000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>242000</td>\n",
       "      <td>0.740000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>243000</td>\n",
       "      <td>0.716300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=243750, training_loss=0.818598441005609, metrics={'train_runtime': 60201.9717, 'train_samples_per_second': 32.391, 'train_steps_per_second': 4.049, 'total_flos': 5.130803778048e+17, 'train_loss': 0.818598441005609, 'epoch': 3.0})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_full.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "541b21c4-6ff7-441d-89c6-cb5700d5b265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2500/2500 03:24]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.7936536073684692,\n",
       " 'eval_accuracy': 0.6699,\n",
       " 'eval_runtime': 204.7246,\n",
       " 'eval_samples_per_second': 97.692,\n",
       " 'eval_steps_per_second': 12.212,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_full.evaluate(full_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "23c690e9-a5ed-47dd-af50-1af74dee2a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_full.save_model(f\"{model_dir}/full_finetuned-trainer\")\n",
    "trainer_full.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7900a0a-9d6a-44b9-8516-f92b0f382d0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
